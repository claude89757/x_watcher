#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time    : 2024/8/11 23:08
@Author  : claude
@File    : 3_AI_Analyze_Data.py
@Software: PyCharm
"""

import os
import time
import datetime

import pandas as pd
import requests
import streamlit as st
from io import StringIO

from common.config import CONFIG
from common.log_config import setup_logger

# Configure logger
logger = setup_logger(__name__)


def send_text_to_gpt(model: str, system_prompt: str, data: pd.DataFrame, batch_size: int = 1000) -> pd.DataFrame:
    """
    å‘é€æ•°æ®åˆ°GPTæ¨¡å‹ï¼Œè·å–åˆ†æç»“æœã€‚
    :param system_prompt: ç³»ç»Ÿçº§æŒ‡ä»¤ï¼Œç”¨äºæä¾›åˆ†æä¸Šä¸‹æ–‡ã€‚
    :param data: åŒ…å«éœ€è¦åˆ†æçš„æ•°æ®çš„DataFrameã€‚
    :param batch_size: æ¯æ¬¡å‘é€çš„æ•°æ®è¡Œæ•°ï¼Œé»˜è®¤1000è¡Œã€‚
    :return: åŒ…å«åˆ†æç»“æœçš„DataFrameã€‚
    """
    results = []
    max_tokens = 2000  # å‡è®¾æ¯è¡Œè¾“å…¥å’Œè¾“å‡ºéƒ½å¾ˆé•¿

    # åˆå§‹åŒ– Streamlit è¿›åº¦æ¡å’Œæ–‡æœ¬æ˜¾ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()

    # è®¡ç®—æ€»æ‰¹æ¬¡æ•°é‡
    total_batches = len(data) // batch_size + (1 if len(data) % batch_size != 0 else 0)

    for i in range(0, len(data), batch_size):
        batch = data.iloc[i:i + batch_size]
        batch_csv = batch.to_csv(index=False)

        batch_prompt = f"{system_prompt}\n\n" \
                       f"Analyze the following data and provide the output in CSV format " \
                       f"with the following structure:" \
                       f"\n1. Original data with all columns intact." \
                       f"\n2. A new column named 'Analysis Explanation' " \
                       f"with insights or explanations for each row based on the data." \
                       f"\n3. A new column named 'Classification Tag' with a category or tag indicating " \
                       f"the potential interest level of each row in product XYZ." \
                       f"\n\nData:\n{batch_csv}"

        payload = {
            "messages": [
                {"role": "system", "content": batch_prompt}
            ],
            "temperature": 0.7,
            "top_p": 0.95,
            "max_tokens": max_tokens
        }

        url = f"https://chatgpt3.openai.azure.com/openai/deployments/{model}/chat/completions?" \
              f"api-version=2024-02-15-preview"
        response = requests.post(url,
                                 headers={
                                     "Content-Type": "application/json",
                                     "api-key": CONFIG.get("azure_open_api_key"),
                                 },
                                 json=payload
        )
        response.raise_for_status()

        response_content = response.json()['choices'][0]['message']['content']

        if "```csv" in response_content:
            csv_content = response_content.split("```csv")[1].split("```")[0].strip()
        else:
            csv_content = response_content

        # Debugging: Print the CSV content to verify format
        print("CSV Content:", csv_content)

        # Handle potential parsing errors
        try:
            csv_rows = csv_content.splitlines()
            reader = pd.read_csv(StringIO("\n".join(csv_rows)), sep=",", iterator=True, chunksize=batch_size)

            for chunk in reader:
                results.append(chunk)

        except pd.errors.ParserError as e:
            st.error(f"Error parsing CSV data: {e}")
            continue

        # æ¯æ¬¡å¤„ç†å®Œä¸€æ‰¹åæ›´æ–°è¿›åº¦æ¡å’ŒçŠ¶æ€ä¿¡æ¯
        progress_percentage = (i + batch_size) / len(data)
        progress_bar.progress(min(progress_percentage, 1.0))
        status_text.text(
            f"Batch {i // batch_size + 1}/{total_batches}: Sent {len(batch)} rows, received {len(csv_rows) - 1} rows.")

    result_df = pd.concat(results, ignore_index=True)

    # å¤„ç†å®Œæˆåï¼Œæ¸…é™¤çŠ¶æ€ä¿¡æ¯
    progress_bar.empty()
    status_text.text("Processing completed!")

    return result_df


# set page config
st.set_page_config(page_title="Analyze Data", page_icon="ğŸ¤–", layout="wide")

# init session state
if 'access_code' not in st.session_state:
    st.session_state.access_code = st.query_params.get('access_code')
if "search_keyword" not in st.session_state:
    st.session_state.search_keyword = st.query_params.get("search_keyword")
if "selected_file" not in st.session_state:
    st.session_state.selected_file = ""
if "analysis_run" not in st.session_state:
    st.session_state.analysis_run = False

# check access
if st.session_state.access_code and st.session_state.access_code in CONFIG['access_code_list']:
    st.query_params.access_code = st.session_state.access_code
else:
    st.warning("Access not Granted!")
    time.sleep(3)
    st.switch_page("Home.py", )

# å¼ºåˆ¶å“åº”å¼å¸ƒå±€
st.write(
    """<style>
    [data-testid="column"] {
        width: calc(50% - 1rem);
        flex: 1 1 calc(50% - 1rem);
        min-width: calc(50% - 1rem);
    }
    </style>""",
    unsafe_allow_html=True,
)

# éšè—Streamlitå…ƒç´ 
hide_streamlit_style = """
            <style>
            .stDeployButton {visibility: hidden;}
            [data-testid="stToolbar"] {visibility: hidden !important;}
            footer {visibility: hidden !important;}
            </style>
            """
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

st.title("Step 3: AI Analyze Data")
st.markdown("Sending data to a large model for analysis, where the model will process "
            "and generate insights based on the provided data.")

src_dir = f"./data/{st.session_state.access_code}/processed/"
dst_dir = f"./data/{st.session_state.access_code}/analyzed/"
files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]
# è·å–é»˜è®¤æ–‡ä»¶åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•
if st.session_state.selected_file and st.session_state.selected_file in files:
    default_index = files.index(st.session_state.selected_file)
else:
    default_index = 0  # å¦‚æœé»˜è®¤æ–‡ä»¶ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶
st.session_state.selected_file = st.selectbox("Select a file to analyze:", files, index=default_index)
selected_file_path = None
if st.session_state.selected_file:
    selected_file_path = os.path.join(src_dir, st.session_state.selected_file)
    st.subheader(f"File Data Preview: {st.session_state.selected_file}")
    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ–‡ä»¶
    try:
        # è·å–æ–‡ä»¶ä¿¡æ¯
        data = pd.read_csv(selected_file_path)
        file_size = os.path.getsize(selected_file_path)  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        file_mod_time = datetime.datetime.fromtimestamp(os.path.getmtime(selected_file_path))  # æ–‡ä»¶ä¿®æ”¹æ—¶é—´

        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        st.write(f"File Size: {file_size / 1024:.2f} KB")  # è½¬æ¢ä¸º KB
        st.write(f"Number of Rows: {data.shape[0]}")
        st.write(f"Last Modified Time: {file_mod_time.strftime('%Y-%m-%d %H:%M:%S')}")

        if data is not None:
            st.dataframe(data.head(500))
        else:
            st.write("No data to display.")
    except Exception as e:
        st.error(f"Error loading data from local file: {e}")
else:
    st.warning("No processed data, return to filter data...")
    time.sleep(3)
    st.switch_page("pages/2_Filter_Data.py")

prompt = st.text_area("Enter your prompt for analysis:",
                      value="Analyze the data and identify potential customers interested in purchasing product XYZ")

# é€‰æ‹©æ¨¡å‹
selected_model = st.selectbox("LLM Model:",  ["gpt-4o-mini", "gpt-4o"])

# æ ¹æ®æ˜¯å¦å·²ç»è¿è¡Œåˆ†ææ¥ç¡®å®šæŒ‰é’®æ ‡ç­¾
analyze_button = st.button("Start Analysis" if not st.session_state.analysis_run else "Reanalyze", type="primary")

# åœ¨åˆ†æä¹‹å
if analyze_button:
    with st.spinner('Analyzing data...'):
        data = pd.read_csv(selected_file_path)
        st.write("Data loaded successfully. Starting analysis...")

        if prompt:
            result_df = send_text_to_gpt(selected_model, prompt, data)

            if not result_df.empty:
                # å®šä¹‰æ–°çš„è¾“å‡ºç›®å½•
                output_file = os.path.join(dst_dir, f"{st.session_state.selected_file}")
                # ä¿å­˜åˆ†æç»“æœ
                result_df.to_csv(output_file, index=False)

                # æ˜¾ç¤ºç»“æœ
                st.success(f"Analysis complete! Results saved to {output_file}.")
                st.write("## Analysis Results")
                st.write(result_df)
            else:
                st.error("Failed to generate analysis results. Please check your prompt or API settings.")
        else:
            st.warning("Prompt cannot be empty. Please provide a valid prompt.")
